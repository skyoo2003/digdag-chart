## String to partially override influxdb.fullname template (will maintain the release name)
##
# nameOverride:

## String to fully override influxdb.fullname template
##
# fullnameOverride:

## Optionally specify an array of imagePullSecrets.
## Secrets must be manually created in the namespace.
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
# imagePullSecrets:
# - myRegistryKeySecretName

## Digdag image
## ref: https://hub.docker.com/r/szyn/docker-digdag/tags
##
image:
  repository: docker.io/szyn/docker-digdag
  tag: 0.9.42
  ## Specify a imagePullPolicy
  ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
  ##
  pullPolicy: IfNotPresent
  # command:
  args:
    - "server"
    - "--config"
    - "/opt/etc/digdag/server.properties"

## Number of Digdag replicas to deploy
##
replicaCount: 2

## Update strategy to deploy
## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
##
updateStrategy:
  type: RollingUpdate

## Duration in seconds a Digdag pod needs to terminate gracefully.
##
terminationGracePeriodSeconds: 30

## Whether to share process namespace for pods
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/share-process-namespace/
shareProcessNamespace: false

## Pod's host aliases
## ref: https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
##
hostAliases: []

## Array to add volumes
##
volumes: []

## Array to add extra volume mounts
##
volumeMounts: []

## Array to environment variables
##
envs: []

## Pod annotations
## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
##
podAnnotations: {}

## Pod Security Context.
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
##
podSecurityContext:
  enabled: false
  fsGroup: 1000

## Pod containers' Security Context
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
##
securityContext:
  enabled: false
  # capabilities:
  #   drop:
  #   - ALL
  runAsNonRoot: true
  runAsUser: 1000

## Role-based access control (RBAC) configuration
## ref: https://kubernetes.io/docs/reference/access-authn-authz/rbac/
##
rbac:
  enabled: false

## Service account for Digdag to use.
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
##
serviceAccount:
  # Specifies whether a service account should be created
  create: false
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

## Node labels for pod assignment
## ref: https://kubernetes.io/docs/user-guide/node-selection/
#
nodeSelector: {}

## Tolerations for pod assignment
## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
##
tolerations: []

## Affinity for pod assignment
## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
##
affinity: {}

## Digdag Service
##
service:
  ## Digdag service type
  ##
  type: ClusterIP

  ## Digdag service port number
  ##
  port: 65432

  ## Specify the nodePort value for the LoadBalancer and NodePort service types.
  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport
  ##
  # nodePort: 30000

  ## Set the LoadBalancer service type to internal only.
  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
  ##
  # loadBalancerIP:
  # loadBalancerSourceRanges:
  # - 10.10.10.0/24

  ## Provide any additional annotations which may be required. This can be used to set the LoadBalancer service type to internal only.
  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
  ##
  annotations: {}

jmx:
  enabled: false
  port: 8999

ingress:
  enabled: false

  ## For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
  ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
  # ingressClassName: nginx

  labels: {}

  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"

  hosts: []
  # - host: digdag.local
  #  paths: []
  #  - path: /

  tls: []
  # - secretName: digdag-ingress-tls
  #   hosts:
  #     - digdag.local

resources: {}
  # limits:
  #   cpu: 500m
  #   memory: 1024Mi
  # requests:
  #   cpu: 500m
  #   memory: 1024Mi

livenessProbe:
  enabled: true
  initialDelaySeconds: 60
  periodSeconds: 10
  timeoutSeconds: 5
  successThreshold: 1
  failureThreshold: 6

readinessProbe:
  enabled: true
  initialDelaySeconds: 60
  periodSeconds: 10
  timeoutSeconds: 5
  successThreshold: 1
  failureThreshold: 6

configuration: |-
  server.bind=0.0.0.0
  server.port=65432
  {{- if .Values.jmx.enabled }}
  server.jmx.port={{ .Values.jmx.port }}
  {{- end }}
  database.type=postgresql
  database.maximumPoolSize=64
  database.user=digdag
  database.password=digdag
  database.host={{ .Release.Name }}-postgresql-ha-pgpool
  database.port=5432
  database.database=digdag_db
  database.ssl=false
  database.migrate=false
  digdag.secret-encryption-key=MTZfYnl0ZXNfcGhyYXNlIQ==
  executor.task_ttl=10d
  executor.attempt_ttl=30d

## Additional sidecar containers
##
# sidecars:
#   - name: container
#     image: image
#     imagePullPolicy: IfNotPresent
sidecars: []

## InitContainer for waiting PostgreSQL is ready.
##
waitForIt:
  image:
    repository: docker.io/willwill/wait-for-it
    tag: latest
    pullPolicy: Always
    # command:
    args:
      - "--timeout=0"
      - "{{ .Release.Name }}-postgresql-ha-pgpool:5432"
      - "--"
      - "echo"
      - "PostgreSQL wait completed"

metrics:
  enabled: false

  image:
    repository: docker.io/sscaling/jmx-prometheus-exporter
    tag: 0.12.0
    pullPolicy: IfNotPresent

  ## JMX exporter configuration
  ## See https://github.com/prometheus/jmx_exporter#configuration
  #
  configuration: |-
    startDelaySeconds: 0
    hostPort: 127.0.0.1:{{ .Values.jmx.port }}
    ssl: false
    lowercaseOutputName: false
    rules:
    - pattern: "com.zaxxer.hikari<type=Pool \\(.+\\)><>(\\w+): (\\d+)"
      name: digdag_hikari_$1
      type: GAUGE
      value: $2
      help: "Digdag HikraiCP $1 $2"
      attrNameSnakeCase: true

  resources: {}
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  serviceMonitor:
    ## If true, a ServiceMonitor CRD is created for a prometheus operator
    ## https://github.com/coreos/prometheus-operator
    ##
    enabled: false
    # Namespace for a ServiceMonitor resource (defaults to use the namespace this chart is deployed to)
    # namespace: monitoring
    interval: 10s
    scrapeTimeout: 10s
    # labels: {}
    #   prometheus: kube-prometheus
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "9187"
    relabelings: []
    metricRelabelings: []

  ## PrometheusRule configuration
  ## ref: https://github.com/coreos/prometheus-operator#customresourcedefinitions
  ##
  prometheusRule:
    enabled: false
    namespace: ""
    ## PrometheusRule custom rules
    ## ref: https://docs.openshift.com/container-platform/4.4/rest_api/monitoring_apis/prometheusrule-monitoring-coreos-com-v1.html
    ##
    rules:
      - alert: TooManyConnections
        expr: digdag_hikari_active_connections{service="{{ template "digdag.fullname" . }}"} / digdag_hikari_total_connections{service="{{ template "digdag.fullname" . }}"} > 0.5
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: Too many connections (instance {{ "{{ $labels.instance }}" }})
          description: |-
            Digdag instance has too many connections ratio (> 0.5)
            VALUE = {{ "{{ $value }}" }}
            LABELS = {{ "{{ $labels }}" }}

  livenessProbe:
    enabled: true
    initialDelaySeconds: 60
    periodSeconds: 10
    timeoutSeconds: 5
    successThreshold: 1
    failureThreshold: 6

  readinessProbe:
    enabled: true
    initialDelaySeconds: 60
    periodSeconds: 10
    timeoutSeconds: 5
    successThreshold: 1
    failureThreshold: 6

## Horizontal Pod Autoscaler configuration
## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
##
autoscaling:
  enabled: false

  ## Custom HPA scaling up/down rules
  ## ref: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.20/#hpascalingrules-v2beta2-autoscaling
  # behavior:
  #   scaleDown:
  #   scaleUp:

  ## Maximum number of replicas to which the autoscaler can scale up. It cannot be less that minReplicas.
  maxReplicas: 10

  ## Minimum number of replicas to which the autoscaler can scale down.
  minReplicas: 2

  ## Target CPU utilization (%)
  cpuUtilization: 50

  ## Target Memory utilization (%)
  memoryUtilization: 50

## Pod Disruption Budget
## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
##
pdb:
  enabled: false

  ## Min number of pods that must still be available after the eviction
  ##
  minAvailable: 1

  ## Max number of pods that can be unavailable after the eviction
  ##
  # maxUnavailable: 1

  ## Annotations to add to the pod disruption budget
  ##
  annotations: {}

## Role Based Access Control
## ref: https://kubernetes.io/docs/reference/access-authn-authz/rbac
rbac:
  create: false

## Postgresql HA Subchart's values
## ref: https://github.com/bitnami/charts/tree/master/bitnami/postgresql-ha
##
postgresql-ha:
  enabled: true

  postgresql:
    initdbScripts:
      create-db.sql: |-
        CREATE USER digdag WITH PASSWORD 'digdag' SUPERUSER;
        CREATE DATABASE digdag_db OWNER digdag;
    repmgrPassword: repmgr
    maxConnections: 120
    replicaCount: 2

  pgpool:
    maxPool: 1
    numInitChildren: 100
    logConnections: true
    clientIdleLimit: 0
    childLifeTime: 0
    customUsers:
      usernames: digdag
      passwords: digdag
    replicaCount: 2

  persistence:
    enabled: false
    size: 8Gi

## Postgresql Subchart's values (without PGPool)
## ref: https://github.com/bitnami/charts/tree/master/bitnami/postgresql
##
postgresql:
  enabled: false

  initdbScripts:
    create-db.sql: |-
      CREATE USER digdag WITH PASSWORD 'digdag' SUPERUSER;
      CREATE DATABASE digdag_db OWNER digdag;

  postgresqlMaxConnections: 100

  persistence:
    enabled: false
    size: 8Gi

testFramework:
  enabled: false
